[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/Tsn2r_bP)
# RL Lecture - Exercise 01

![plot](./images/bandit_resized.png)


## Task 1: Exploration in Multi-Armed Bandits

In this exercise, you are tasked with implementing various exploration strategies in `expercise01.py`. The strategies include:

- UCB (Upper Confidence Bound)
- Softmax Action Selection
- $\epsilon$-greedy
- Random exploration
- Decaying $\epsilon$-greedy (with a linear decaying schedule)

After implementing these strategies, experiment with different hyperparameters. Evaluate their performance on the provided toy problem by plotting the regret and compare the results between strategies.

## Task 2: Experiences

Share your experience! Provide a brief summary of your experience with this exercise and the corresponding lecture in feedback.md.
Optionally, you can also make a post in the discussion thread **Exercise 01: Introduction & Bandits** on the [forum](https://ilias.uni-freiburg.de/goto.php?target=frm_3633835&client_id=unifreiburg).

---

### Submission Instructions

- Implement the strategies in `exercise01.py`.
- Create a graph plotting the regret for each strategy and compare them.
- Share your experiences in the forum.
- For any questions or problems regarding your solutions please let us know through `feedback.md` so we can give you specific feedback.

---

**Supervisors:**  
Prof. Joschka Boedecker, Dr. Gabriel Kalweit, Philipp Bordne, Julien Brosseit, Jasper Hoffmann, Yuan Zhang 
